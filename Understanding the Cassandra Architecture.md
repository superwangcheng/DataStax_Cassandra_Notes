# Understanding the Cassandra architecture

## Cassandra node-based architecture

* A cluster is a p2p set of nodes with no single point of failure
	- **a node** is a cassandra instance (in production: one node per machine)
	- **a partition** is one ordered and replicable unit of data on a node
	- **a rack** is a logical set of nodes
	- **a Data Center** is a logical set or racks
	- **Cluster** is the full set of nodes which map to a single complete **token ring**


* How do nodes recognize their cluster?
	- Nodes join a cluster based on the configuration of their own `conf/cassandra.yaml` file. Key settings are :
	- **seed**: IP addresses of initial nodes for a new node to contact and discover the cluster topology, best practice to use the same two per data center
	- **cluster_name**: shared name to logically distinguish a set of nodes)
	- **listen_address**: IP address through which this particular node communicates

## Understanding request coordination

A **coordinator** is the node chosen by the client to receive a particular read or write request to its cluster. It will dispatch the read/write request toward the cluster (to 1 or multiple node) and receive the aknowledgment of that request.
* **Any node can coordinate any request**
* **Each client request may be coordinated by a different node**
* **No Single point of failure: fundamental to Cassandra's architecture**

How are client request coordinated: the **Cassandra Driver** chooses the node to which each read or write request is sent
* Client library providing APIs to manage client read/write requests
* Round-robin pattern by default

DataStax maintains open source drivers (Java, Python, C++, C# ...)

The **coordinator** manages the replicaton process:
* **Replication factor (RF)**: onto how many nodes should a write be copied?
* Every partition is a **replica** (even if only one)
* Possible values for RF range from 1 to the total of planned nodes for the cluster
* **Replication strategy**: determines which nodes receive replicas

Every write to every node is individually time-stamped

The coordinator also applies the **Consistency level (CL)**
* **Consistency level (CL)**: how many nodes must aknowledge a read or write request
* CL may vary for each request
* on success, coordinator notifies client
* possible CL: ANY, ONE, QUORUM (RF/2+1), ALL

## Partitionning process

Data is stored on nodes in **partitions**, each identified by a **token**
* **Token**: a 128 bit integer ID used to identify each partition and node
* **Partition**: a unit of data storage on a node (analogous to a 'row')
* **Partition key**: a token calculated to identify a specific partition
The 2^127-1 value token range for a cluster is treated as a ring

Example: imagine 0 to 100 token range:
* Each node in a cluster is identified by the highest token in one segment of the token range
	- 4 nodes with tokens 0, 25, 50, 75
* Each node is primarily responsible for partitions with partition keys (tokens) in the preceding segment (ex. node with token 25 is responsible for partition keys from 1 to 25)
	- does not means that it is the only possible node where the partition could exist (data is replicated!)
	- node that owns the primary range for the partition key of a given partition is where the first replica goes.

Cassandra knows where to place the first replica and then calculates based on replication strategies where the next replica goes.

**How keys are generated for new partitions**: Partition keys are generated by **partitionners**
* **Partitionners**: a system *on each node* hashing primary key values into a token to be used as a partition key
* **Hash function**: converts a variable length value to a corresponding fixed length value.
* A table's primary key sets the values to be hashed into tokens for use as partition key.

Example: consider the table created by `CREATE TABLE Users (firstname text, lastname text, level text, PRIMARY KEY(lastname,firstname));`
The primary key is used to determine partition key. So if we do `INSERT INTO Users (firstname,lastname,level) VALUES ('oscar','orange',42)`, "orange, oscar" is passed to the partitionner and token value is calculated, then coordinator determines which node hold the primary range for this token.

Cassandra offers 3 partitionners (configured in `cassandra.yaml`, must be the same accross all nodes!):
* Murmur3Partitioner (default): uniform distribution
* RandomPartitionner
* ByteOrderedPartitioner (legacy)

## Virtual nodes

So far we talked about nodes in continuous segments of token in the token ring.

In fact, multiple smaller segments can be owned by a node, as its primary range, instead of one large segment. Each of these smaller segments, called virtual nodes, handles read and write just as a regular node.

* **Automates assignment of token range** (before it was necessary to calculate what token should be the initial token of each node)
* virtual nodes **bootstrap** and **decomission** more quickly
	- **bootstrap**: node joins cluster and fills with partitions for its primary range (other nodes 'give' partition to the new node).
	- **decommission**: node leaves a cluster and drains partitions to other nodes taking responsibility for its primary range. Impact of node failure is minimized, other nodes compensate more quickly.

	Virtual nodes are configured in `cassandra.yaml`.

	* enabled by default in Cassandra 2.0+.
	* set *num_tokens* to specify the number of token ranges per node (default 256)
	* not used for nodes implementing DSE Search or DSE Analytics features.

## Replication

The target table's *keyspace* determines both:
* **The replication factor**: how many replica to make of each partition
* **The replication strategy**: on which node should each replica be placed

All partitions are "replicas", there are no "original".
First replica is placed on the node owning the token's primary range.
Then the replication strategies determines where the next replica go.

Each node belong to a rack in one data center (a default Cassandra node is in rack "RAC1" and data center "DC1"). This enables geographically aware read and write request routing (topology is communicated to Partitioner by *Snitch* and possibly *Gossip* systems). Configuration file varies based on chosen *Snitch*.

**replication strategy** and **replication factor** are configured as part of declaring a new keyspace.

**SimpleStrategy**: one factor for entire cluster (learning use only), assigned as "replication_factor". There will be 2 replica and the 2nd will be made on node subsequent to first replica. Example:
```
CREATE KEYSPACE simple_demo
WITH REPLICATION =
{'class':'SimpleStrategy',
 'replication_factor':2}
```

This is not good if we have multiple datacenter because all replica would end up in the same datacenter.

**NetworkTopologyStrategy**: unique factor for each data center. Assigned by data center id (beyond the scope of this class).
```
CREATE KEYSPACE simple_demo
WITH REPLICATION =
{'class':'NetworkTopologyStrategy',
 'dc-east':2, 'dc-west':3}
```

In that case a remote coordinator is named in the other datacenter.

## Hinted handoff

A feature which add to the durability of a Cassandra cluster.
A recovery mechanism for writes targeting offline nodes.
Coordinator can store a *hinted handoff* it target node for a write is known o be down or fails to acknowledge. The coordinator stores the hint in its `system.hints` table. The write is replayed when the node comes back online.

Hinted handoff is comprised of:
* address of the target node which is down
* partition and data requiring a replay

Configuration in the `cassandra.yaml` file:
* *hinted_handoff_enabled* (default true)
* *max_hint_window_in_ms* (default 3 hours): after this consecutive outage period hints are no longer generated until target node comes back online (i.e. something is definitly going on with this node, we'll use nodetool repair).

## Consistency levels

Partition *key*, *replication factor* and *replication strategy* determine which node are sent any given request.

**Consisteny level** determines how many nodes must aknowledge the request because response is returned to the client/driver.
* CL may vary for each request
* on success, coordinator notifies client
* possible
	- ANY: write to any node (highest availability, lowest consistency)
	- ONE: check closest replica node (highest availability, lowest consistency)
	- QUORUM (RF/2+1) (balanced consistency and availability)
	- ALL: check all replica, fail if any is down

Meaning vaies by request :
* *write request*: how many nodes must acknowledge they received and wrote the partition?
* *read request*: how many replica nodes must aknowledgeand send their most recent partition data? (read results merge to most current by coordinator)

**By default consistency level for all requests is ONE**
* in cql shell the CONSISTENCY command set the value for all subsequent requests during the session
* When working with client drivers, a *ConsistencyLevel* constant is passed as part of each request

## Tunable consistency

**Immediate vs eventual consistency:**

* **immediate consistency**: Reads always return the most recent data.
	- consistency levels all guarantees immediate consistency, because all replicas are read and compared before a result is returned.
	- *highest latency* because all replica nodes are checked and compared
* **Eventual consistency**: reads may return stale data.
	- consistency level ONE is very fast, because the replica from the first node to respond is immediately returned, but carries the highest risk of stale data.
	- Lowest latency because the first replica to reach the coordinator is returned.

Reads and writes may each be set to a specific consistency level.
* Write consistency
* Read consistency

**If (nodes_written + nodes_read) > replication factor then we have immediate consistency**. But we have the oppourtunity to achieve this immediate consistency by
* high write consistency (write CL = ALL / read CL = 1)
* or high read consistency (read CL = ALL, write CL = 1)
* or a balanced approach (write CL = QUORUM, read CL = QUORUM)

**Clock synchronization across nodes is critical because**:
* Every write to any column includes the column name, value and timestamp
* Reads merge the most recent values from each node into one response

**How do we choose consistency level ?**

Is the value of immediate consistency worth the latency cost?
* Netflix uses CL = ONE and measures its "eventual" consistency in milliseconds
* consistency level ONE "is your friend"

CL ONE : Lowest latency + Highest throughput + Highest availability and stale read possible
CL QUORUM: higher latency than ONE, Lower throughput, Higher availability (then ALL)
and no stale reads.
CL ALL: Highest latency + Lowest throughput + Lowest availability and No stale read.

**Conclusion: Is "Stale" is measured in milliseconds, how much are those milliseconds worth?**


## Introducing repair operation

Cassandra's architecture allows for temporary data inconsistency
* actual amount dependson how consistency levels are set for read/write
* tuning requests for immediate consistency minimizes need for other repair

Inconsistencies among nodes are repaired multiple ways:
* **Read repair** (readtime): automatic data repartition updates during reads
* **nodetool repair** (maintenance time): periodic full node repairs

Repair operations are also known as anti-entropy operations.

**Read repair**:
* During each read request:
	- full data is requested from one replica node
	- a **digest query** is sent to all other replica nodes
	- **digest query**: returns a hash to check the current data state, rather than return a complete query result.
	- nodes with stale data are updated
* This happens on a configurable percentage of reads (not all reads by default)
	- *read_repair_chance* (default 0.1). This is a table property (defined uniquely for each table). Table property can be altered using `cqlsh> ALTER TABLE [table] WITH [property] = [value];`
* Detailed example : Assume nodes 1 to 4 with replication factor = 3 and read consistency level = ALL. Node 4 is the coordinator:
	- it requests the full data from node 1. Digest queries are sent to nodes 2 and 3.
	- nodes 2 and 3 only return only data state.
	- coordinator realizes node 3 has data with a more recent time stamp.
	- coordinator take value from node 3 and merge it into the result
	- **stale nodes 1 and 2 are updated**
* This means that all the nodes are made consistent through the read operation itself.

**Nodetool repair**:
* Makes all data on a node consistent with the most current replicas in the cluster.
* `bin/nodetool -h [host] -p [JMX port] repair [options]`
* Options help mitigate heavy disk use from this operation
	- `-partitionner-range`: option restricts repair to node's primary range only
	- `-start-token [uuid] -end-token [uuid]`: restrict repair to this token range
	- `dc-name[name]` or `-local`: repair named data center, or local center only

**When to run nodetool repair**:
* recovering failed node
* bringing a downed node back online
* periodically on nodes with infrequently read data (not much read repair process on these nodes)
* periodically on nodes with write or delete activity
* periodically every *gc_grace_seconds*:
	- **gc_grace_seconds** (default 864000, 10 days) table property controlling the *tombstone* garbage collection period.
	- **tombstone**: marker placed on a deleted column within a partition
	- see next courses for more details

## Internode communications

**The Gossip protocol** :
* A continuous lightweight internode communication.
	- Once per second, each node contacts 1 to 3 others, requesting and sharing updates about:
	- Is a node availeble? Has it restarted?
	- Where is it? How is it load?
	- Is it available or bootstrapping/decommissioning?
* This is used to detect failed nodes
	- "failure" sensitivity is adjustable
* All Updates are time-stamped and locally persisted
	- history used to restore state on restart
	- purge history if node address changes

Implication: if we change th IP address of a node, then we'll have to purge the history.

**As a node joins the cluster, it gossips with the seed nodes set in its `cassandra.yaml` config file to learn its cluster's topology**
Best practices:
* Assign the same seed nodes to each node in each data center
* If more than one data center, include a seed node from each

**Snitch**: a node sub-system to track and report cluster topology
* Informs the local partitionner about their *rack* and *data center* location
* help enable replication without duplication within a rack (duplicate replica within a rack risk data loss if that rack fails)
* configured by assigning *endpoint_snitch* in `cassandra.yaml`
	- Java class implementing the *IEndpointsSnitch* interface
	- several options distributed with cassandra, or may customize
	- **if we change the Snitch after data is in the cluster, we must run a full cluster repair, because snitch affects where replicas are placed.**

Seven snitch options are distributed with cassandra:
* *SimpleSnitch*: node proximity determined by the strategy declared for the keyspace, single data center only
* *PropertyFileSnitch*: node proximity determined by rack and data center configuration in `cassandra-topology.properties`
* *GossipingPropertyFileSnitch*: node proximity determined by rack and data center configuration in `cassandra-rackdc.properties`, and propagated by Gossip
* *Ec2Snitch*: Amazon EC2 aware ...
* *Ec2MultiRegionSnitch*
* *YamlFileNetworkTopologySnitch*: useful for mixed-cloud clusters, configured using `cassandra-topology.yaml`
* *RackInferringSnitch*: sample for writting a custom Snitch

## Surveying the system keyspace


Cassandra stores its state in **System keyspace tables**
* Examining System keyspace tables is usefull towards understanding Cassandra
* Use the CQL `DESCRIBE KEYSPACE` command to list all System table chema

**do not directly edit any system keyspace**

System keyspace tables include:
* *schema_keyspaces*: Keyspaces available within this cluster, along with their assigned replication strategy and replication factor
* *schema_columns*: Primary key column definitions
* *schema_columnfamilies*: - Tables (column families) and their configuration
* *peers*: Local copies of cluster-wide gossip for this node
* *local*: Details of the local node's own state
* *hints*:  Stores information for hinted handoffs
* and others...
